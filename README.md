üìå Image Captioning with ViT + GPT-2
1. B√†i to√°n

Sinh ch√∫ th√≠ch ·∫£nh (Image Captioning): cho m·ªôt ·∫£nh ƒë·∫ßu v√†o, m√¥ h√¨nh sinh ra m·ªôt c√¢u m√¥ t·∫£ b·∫±ng ti·∫øng Anh.

Encoder: Vision Transformer (ViT) ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh.

Decoder: GPT-2 ƒë·ªÉ sinh c√¢u m√¥ t·∫£ t·ª± nhi√™n, m·∫°ch l·∫°c.

Ki·∫øn tr√∫c: Encoder‚ÄìDecoder d·ª±a tr√™n VisionEncoderDecoderModel c·ªßa Hugging Face.

D·ªØ li·ªáu hu·∫•n luy·ªán: Flickr8k (m·ªói ·∫£nh c√≥ 1‚Äì5 caption ti·∫øng Anh).

2. D·ªØ li·ªáu & Ti·ªÅn x·ª≠ l√Ω

·∫¢nh: resize v·ªÅ (224,224), chuy·ªÉn tensor, chu·∫©n h√≥a (mean=0.5, std=0.5), tr√≠ch ƒë·∫∑c tr∆∞ng b·∫±ng ViTFeatureExtractor.

Ch√∫ th√≠ch: tokenize b·∫±ng GPT-2 tokenizer, pad ƒë·∫øn max_length=50, b·ªè qua token pad trong loss.

Chia d·ªØ li·ªáu: Train 80% ‚Äì Val 20%.

3. Hu·∫•n luy·ªán

M√¥ h√¨nh: VisionEncoderDecoderModel.from_encoder_decoder_pretrained()

Tham s·ªë sinh caption:

max_length=128, num_beams=4, no_repeat_ngram_size=3, length_penalty=2.0.

C·∫•u h√¨nh hu·∫•n luy·ªán (Seq2SeqTrainer):

Epochs: 3

Batch size: 8

Learning rate: 5e-5

Warmup steps: 1024

Evaluation: theo t·ª´ng epoch

L∆∞u duy nh·∫•t m√¥ h√¨nh t·ªët nh·∫•t (save_total_limit=1).

4. K·∫øt qu·∫£

Similarity score: ~0.38

M√¥ h√¨nh sinh caption c√≥ √Ω nghƒ©a g·∫ßn gi·ªëng so v·ªõi ground-truth, ng·ªØ ph√°p t·ª± nhi√™n.

5. L√Ω do ch·ªçn ViT + GPT-2

ViT: tr√≠ch xu·∫•t quan h·ªá kh√¥ng gian to√†n c·ª•c t·ªët h∆°n CNN.

GPT-2: sinh ng√¥n ng·ªØ m∆∞·ª£t m√†, c√≥ ng·ªØ nghƒ©a t·ªët.

Hugging Face: d·ªÖ tri·ªÉn khai, t√°i s·ª≠ d·ª•ng v√† tinh ch·ªânh.
