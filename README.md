# ğŸ–¼ï¸ Image Captioning with ViT + GPT-2  

## ğŸ” Giá»›i thiá»‡u  

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh **Deep Learning** cÃ³ kháº£ nÄƒng sinh chÃº thÃ­ch áº£nh (**Image Captioning**) báº±ng tiáº¿ng Anh.  
Há»‡ thá»‘ng káº¿t há»£p:  

- **Vision Transformer (ViT)** ğŸ‘‰ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng áº£nh.  
- **GPT-2** ğŸ‘‰ sinh cÃ¢u mÃ´ táº£ tá»± nhiÃªn.  
- **Framework**: Hugging Face `transformers` + PyTorch.  
- **Dataset**: Flickr8k (áº£nh + caption tiáº¿ng Anh).  

---

## ğŸ‘¨â€ğŸ« ThÃ nh viÃªn  

| Há» vÃ  TÃªn       | MSSV      |
|-----------------|-----------|
| ChÃ¢u Tiá»ƒu Long  | 21094341  |

**GV hÆ°á»›ng dáº«n:** ...  
**TrÆ°á»ng:** Äáº¡i há»c CÃ´ng Nghiá»‡p TP.HCM  

---

## ğŸ¯ Má»¥c tiÃªu  

- XÃ¢y dá»±ng mÃ´ hÃ¬nh sinh chÃº thÃ­ch áº£nh.  
- á»¨ng dá»¥ng Vision Transformer thay CNN Ä‘á»ƒ tÄƒng hiá»‡u quáº£ trÃ­ch xuáº¥t áº£nh.  
- Sá»­ dá»¥ng GPT-2 Ä‘á»ƒ táº¡o caption mÆ°á»£t mÃ  vÃ  Ä‘Ãºng ngá»¯ nghÄ©a.  
- ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng báº±ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá»›i caption gá»‘c.  

---

## ğŸ§° CÃ´ng nghá»‡ & thÆ° viá»‡n sá»­ dá»¥ng  

- **Python, PyTorch, Hugging Face Transformers**  
- **PIL, torchvision** (tiá»n xá»­ lÃ½ áº£nh)  
- **Seq2SeqTrainer** (huáº¥n luyá»‡n Encoderâ€“Decoder)  

---

## ğŸ–¥ï¸ Kiáº¿n trÃºc mÃ´ hÃ¬nh  

### ğŸ”§ Cáº¥u trÃºc Encoderâ€“Decoder  

- **Encoder**: Vision Transformer (ViT) â€“ chia áº£nh thÃ nh patch, trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng toÃ n cá»¥c.  
- **Decoder**: GPT-2 â€“ sinh vÄƒn báº£n mÃ´ táº£.  
- **Huáº¥n luyá»‡n**: `VisionEncoderDecoderModel.from_encoder_decoder_pretrained()`  

### ğŸ“ Cáº¥u hÃ¬nh huáº¥n luyá»‡n  

- Epochs: 3  
- Batch size: 8  
- Learning rate: 5e-5  
- Max caption length: 128  
- Beam search: 4  

---

## ğŸ“Š Káº¿t quáº£  

- **Similarity score**: ~ **0.38**  
- Caption sinh ra ngá»¯ nghÄ©a gáº§n Ä‘Ãºng vÃ  ngá»¯ phÃ¡p tá»± nhiÃªn.  

---

## ğŸ–¼ï¸ VÃ­ dá»¥  

### Input  
<img src="Screenshot 2025-05-05 142134.png" width="800"/>  
### Output Caption  
<img src="TestImage.png" width="800"/>  
